{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNxK6BjblB62/dPUC738MQ6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EmineSener/Computer-Vision-With-Transformer/blob/main/coursera_Assignment3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Image Classification and Object Localization\n",
        "\n",
        "In this lab, you'll build a CNN from scratch to:\n",
        "- classify the main subject in an image\n",
        "- localize it by drawing bounding boxes around it.\n",
        "\n",
        "You'll use the [MNIST](http://yann.lecun.com/exdb/mnist/) dataset to synthesize a custom dataset for the task:\n",
        "- Place each \"digit\" image on a black canvas of width 75 x 75 at random locations.\n",
        "- Calculate the corresponding bounding boxes for those \"digits\".\n",
        "\n",
        "The bounding box prediction can be modelled as a \"regression\" task, which means that the model will predict a numeric value (as opposed to a category)."
      ],
      "metadata": {
        "id": "FVBZeRjTji24"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "NifV3bdikYoI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, re, time, json  # Importing necessary libraries for file operations, regular expressions, time, and JSON handling\n",
        "import PIL.Image, PIL.ImageFont, PIL.ImageDraw  # Importing modules from the Python Imaging Library (PIL) for image processing\n",
        "import numpy as np  # Importing NumPy library for numerical operations\n",
        "\n",
        "try:\n",
        "    # %tensorflow_version only exists in Colab.\n",
        "    %tensorflow_version 2.x  # Checking if the code is running in Google Colab and setting TensorFlow version to 2.x\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "import tensorflow as tf  # Importing TensorFlow library\n",
        "from matplotlib import pyplot as plt  # Importing pyplot module from matplotlib for plotting\n",
        "import tensorflow_datasets as tfds  # Importing TensorFlow Datasets for accessing datasets\n",
        "\n",
        "print(\"Tensorflow version \" + tf.__version__)  # Printing the TensorFlow version"
      ],
      "metadata": {
        "id": "lEQatijikWen",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ad60124-67a8-4b22-bc81-77f5bdbd4b81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n",
            "Tensorflow version 2.15.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualization Utilities\n",
        "\n",
        "These functions are used to draw bounding boxes around the digits."
      ],
      "metadata": {
        "id": "fDXBC0qskqxL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the width of the image\n",
        "im_width = 75\n",
        "# Define the height of the image\n",
        "im_height = 75\n",
        "# Flag indicating whether to use normalized coordinates for bounding boxes\n",
        "use_normalized_coordinates = True"
      ],
      "metadata": {
        "id": "q87Ik2RomNGx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def draw_bounding_boxes_on_image_array(image,\n",
        "                                       boxes,\n",
        "                                       color=[],\n",
        "                                       thickness=1,\n",
        "                                       display_str_list=()):\n",
        "  \"\"\"Draws bounding boxes on image (numpy array).\n",
        "  Args:\n",
        "    image: a numpy array object.\n",
        "    boxes: a 2 dimensional numpy array of [N, 4]: (ymin, xmin, ymax, xmax).\n",
        "           The coordinates are in normalized format between [0, 1].\n",
        "    color: color to draw bounding box. Default is red.\n",
        "    thickness: line thickness. Default value is 4.\n",
        "    display_str_list_list: a list of strings for each bounding box.\n",
        "  Raises:\n",
        "    ValueError: if boxes is not a [N, 4] array\n",
        "  \"\"\"\n",
        "  # Convert the numpy array image to a PIL Image object\n",
        "  image_pil = PIL.Image.fromarray(image)\n",
        "  # Create a new RGBA image\n",
        "  rgbimg = PIL.Image.new(\"RGBA\", image_pil.size)\n",
        "  # Paste the original image onto the new RGBA image\n",
        "  rgbimg.paste(image_pil)\n",
        "  # Call draw_bounding_boxes_on_image function to draw bounding boxes on the image\n",
        "  draw_bounding_boxes_on_image(rgbimg, boxes, color, thickness, display_str_list)\n",
        "  # Convert the RGBA image back to a numpy array\n",
        "  return np.array(rgbimg)"
      ],
      "metadata": {
        "id": "cGh803J2mS3k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def draw_bounding_boxes_on_image(image,\n",
        "                                 boxes,\n",
        "                                 color=[],\n",
        "                                 thickness=1,\n",
        "                                 display_str_list=()):\n",
        "  \"\"\"Draws bounding boxes on image.\n",
        "  Args:\n",
        "    image: a PIL.Image object.\n",
        "    boxes: a 2 dimensional numpy array of [N, 4]: (ymin, xmin, ymax, xmax).\n",
        "           The coordinates are in normalized format between [0, 1].\n",
        "    color: color to draw bounding box. Default is red.\n",
        "    thickness: line thickness. Default value is 4.\n",
        "    display_str_list: a list of strings for each bounding box.\n",
        "  Raises:\n",
        "    ValueError: if boxes is not a [N, 4] array\n",
        "  \"\"\"\n",
        "  # Get the shape of the bounding boxes array\n",
        "  boxes_shape = boxes.shape\n",
        "  # Check if the boxes_shape is empty\n",
        "  if not boxes_shape:\n",
        "    return\n",
        "  # Check if the boxes_shape is not 2D or if the second dimension is not 4\n",
        "  if len(boxes_shape) != 2 or boxes_shape[1] != 4:\n",
        "    raise ValueError('Input must be of size [N, 4]')\n",
        "  # Iterate over each bounding box\n",
        "  for i in range(boxes_shape[0]):\n",
        "    # Call draw_bounding_box_on_image function to draw each bounding box\n",
        "    draw_bounding_box_on_image(image, boxes[i, 1], boxes[i, 0], boxes[i, 3],\n",
        "                               boxes[i, 2], color[i], thickness, display_str_list[i])"
      ],
      "metadata": {
        "id": "xXxLclf1mX3b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def draw_bounding_box_on_image(image,\n",
        "                               ymin,\n",
        "                               xmin,\n",
        "                               ymax,\n",
        "                               xmax,\n",
        "                               color='red',\n",
        "                               thickness=1,\n",
        "                               display_str=None,\n",
        "                               use_normalized_coordinates=True):\n",
        "  \"\"\"Adds a bounding box to an image.\n",
        "  Bounding box coordinates can be specified in either absolute (pixel) or\n",
        "  normalized coordinates by setting the use_normalized_coordinates argument.\n",
        "  Args:\n",
        "    image: a PIL.Image object.\n",
        "    ymin: ymin of bounding box.\n",
        "    xmin: xmin of bounding box.\n",
        "    ymax: ymax of bounding box.\n",
        "    xmax: xmax of bounding box.\n",
        "    color: color to draw bounding box. Default is red.\n",
        "    thickness: line thickness. Default value is 1.\n",
        "    display_str_list: string to display in box\n",
        "    use_normalized_coordinates: If True (default), treat coordinates\n",
        "      ymin, xmin, ymax, xmax as relative to the image.  Otherwise treat\n",
        "      coordinates as absolute.\n",
        "  \"\"\"\n",
        "  # Create a PIL ImageDraw object\n",
        "  draw = PIL.ImageDraw.Draw(image)\n",
        "  # Get the width and height of the image\n",
        "  im_width, im_height = image.size\n",
        "  # Check if normalized coordinates are used\n",
        "  if use_normalized_coordinates:\n",
        "    # Convert normalized coordinates to absolute coordinates\n",
        "    (left, right, top, bottom) = (xmin * im_width, xmax * im_width,\n",
        "                                  ymin * im_height, ymax * im_height)\n",
        "  else:\n",
        "    # Use absolute coordinates\n",
        "    (left, right, top, bottom) = (xmin, xmax, ymin, ymax)\n",
        "  # Draw the bounding box on the image\n",
        "  draw.line([(left, top), (left, bottom), (right, bottom),\n",
        "             (right, top), (left, top)], width=thickness, fill=color)"
      ],
      "metadata": {
        "id": "bHenxkHLkUJn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "These utilities are used to visualize the data and predictions."
      ],
      "metadata": {
        "id": "3kjNHCkpmilS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "This cell contains helper functions used for visualization\n",
        "and downloads only.\n",
        "\n",
        "You can skip reading it, as there is very\n",
        "little Keras or Tensorflow related code here.\n",
        "\"\"\"\n",
        "\n",
        "# Matplotlib configuration for visualization\n",
        "plt.rc('image', cmap='gray')  # Set colormap for images to grayscale\n",
        "plt.rc('grid', linewidth=0)    # Set grid linewidth to 0\n",
        "plt.rc('xtick', top=False, bottom=False, labelsize='large')  # Set properties for x-axis ticks\n",
        "plt.rc('ytick', left=False, right=False, labelsize='large')  # Set properties for y-axis ticks\n",
        "plt.rc('axes', facecolor='F8F8F8', titlesize=\"large\", edgecolor='white')  # Set properties for axes\n",
        "plt.rc('text', color='a8151a')  # Set color for text\n",
        "plt.rc('figure', facecolor='F0F0F0')  # Set facecolor for figures\n",
        "\n",
        "# Set the directory for Matplotlib fonts\n",
        "MATPLOTLIB_FONT_DIR = os.path.join(os.path.dirname(plt.__file__), \"mpl-data/fonts/ttf\")\n"
      ],
      "metadata": {
        "id": "qtVu5S6Wmrg0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jfxdtB6Fm8vH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create digits from local fonts for testing\n",
        "def create_digits_from_local_fonts(n):\n",
        "  font_labels = []\n",
        "  img = PIL.Image.new('LA', (75*n, 75), color = (0,255)) # format 'LA': black in channel 0, alpha in channel 1\n",
        "  font1 = PIL.ImageFont.truetype(os.path.join(MATPLOTLIB_FONT_DIR, 'DejaVuSansMono-Oblique.ttf'), 25)\n",
        "  font2 = PIL.ImageFont.truetype(os.path.join(MATPLOTLIB_FONT_DIR, 'STIXGeneral.ttf'), 25)\n",
        "  d = PIL.ImageDraw.Draw(img)\n",
        "  for i in range(n):\n",
        "    font_labels.append(i%10)\n",
        "    d.text((7+i*75,0 if i<10 else -4), str(i%10), fill=(255,255), font=font1 if i<10 else font2)\n",
        "  font_digits = np.array(img.getdata(), np.float32)[:,0] / 255.0 # black in channel 0, alpha in channel 1 (discarded)\n",
        "  font_digits = np.reshape(np.stack(np.split(np.reshape(font_digits, [75, 75*n]), n, axis=1), axis=0), [n, 75*75])\n",
        "  return font_digits, font_labels\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    plt.imshow(img_to_draw)\n",
        "\n",
        "    if len(iou) > i :\n",
        "      color = \"black\"\n",
        "      if (n_iou[i][0] < iou_threshold):\n",
        "        color = \"red\"\n",
        "      ax.text(0.2, -0.3, \"iou: %s\" %(n_iou[i][0]), color=color, transform=ax.transAxes)\n",
        "\n",
        "\n",
        "# utility to display training and validation curves\n",
        "def plot_metrics(metric_name, title, ylim=5):\n",
        "  plt.title(title)\n",
        "  plt.ylim(0,ylim)\n",
        "  plt.plot(history.history[metric_name],color='blue',label=metric_name)\n",
        "  plt.plot(history.history['val_' + metric_name],color='green',label='val_' + metric_name)"
      ],
      "metadata": {
        "id": "jQ-pB674mjKu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Selecting Between Strategies"
      ],
      "metadata": {
        "id": "FKveWDfZp9gc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TPU or GPU detection\n",
        "\n",
        "Depending on the hardware available, you'll use different distribution strategies.  For a review on distribution strategies, please check out the second course in this specialization [\"Custom and Distributed Training with TensorFlow\"](https://www.coursera.org/learn/custom-distributed-training-with-tensorflow), week 4, \"Distributed Training\".\n",
        "\n",
        "- If the TPU is available, then you'll be using the TPU Strategy.\n",
        "Otherwise:\n",
        "- If more than one GPU is available, then you'll use the Mirrored Strategy\n",
        "- If one GPU is available or if just the CPU is available, you'll use the default strategy."
      ],
      "metadata": {
        "id": "QrRUnR3LqAag"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Detect hardware and select appropriate distribution strategy\n",
        "\n",
        "try:\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
        "except ValueError:\n",
        "    tpu = None\n",
        "    gpus = tf.config.experimental.list_logical_devices(\"GPU\")\n",
        "\n",
        "# Select appropriate distribution strategy based on available hardware\n",
        "if tpu:\n",
        "    # Connect to TPU cluster and initialize TPU system\n",
        "    tf.config.experimental_connect_to_cluster(tpu)\n",
        "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "    # Create a TPU strategy for distributed training\n",
        "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
        "    print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\n",
        "elif len(gpus) > 1:\n",
        "    # Create a MirroredStrategy for multi-GPU training\n",
        "    strategy = tf.distribute.MirroredStrategy([gpu.name for gpu in gpus])\n",
        "    print('Running on multiple GPUs ', [gpu.name for gpu in gpus])\n",
        "elif len(gpus) == 1:\n",
        "    # Use default strategy for single GPU\n",
        "    strategy = tf.distribute.get_strategy()\n",
        "    print('Running on single GPU ', gpus[0].name)\n",
        "else:\n",
        "    # Use default strategy for CPU\n",
        "    strategy = tf.distribute.get_strategy()\n",
        "    print('Running on CPU')\n",
        "\n",
        "# Print the number of replicas (devices) in sync for distributed training\n",
        "print(\"Number of accelerators: \", strategy.num_replicas_in_sync)\n"
      ],
      "metadata": {
        "id": "Jh2t62rqp95h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Parameters\n",
        "\n",
        "The global batch size is the batch size per replica (64 in this case) times the number of replicas in the distribution strategy."
      ],
      "metadata": {
        "id": "Vo4IwJbtrORp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64 * strategy.num_replicas_in_sync # Gobal batch size.\n",
        "# The global batch size will be automatically sharded across all\n",
        "# replicas by the tf.data.Dataset API. A single TPU has 8 cores.\n",
        "# The best practice is to scale the batch size by the number of\n",
        "# replicas (cores). The learning rate should be increased as well."
      ],
      "metadata": {
        "id": "sb7R3MYvrOwC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading and Preprocessing the Dataset\n",
        "\n",
        "Define some helper functions that will pre-process your data:\n",
        "- `read_image_tfds`: randomly overlays the \"digit\" image on top of a larger canvas.\n",
        "- `get_training_dataset`: loads data and splits it to get the training set.\n",
        "- `get_validation_dataset`: loads and splits the data to get the validation set."
      ],
      "metadata": {
        "id": "RUZH3t9KsFQa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Transforms each image in dataset by pasting it on a 75x75 canvas at random locations.\n",
        "'''\n",
        "def read_image_tfds(image, label):\n",
        "    # Randomly generate x and y coordinates for pasting the image\n",
        "    xmin = tf.random.uniform((), 0 , 48, dtype=tf.int32)\n",
        "    ymin = tf.random.uniform((), 0 , 48, dtype=tf.int32)\n",
        "\n",
        "    # Reshape the image to (28, 28, 1)\n",
        "    image = tf.reshape(image, (28,28,1,))\n",
        "\n",
        "    # Pad the image to a 75x75 canvas at the randomly generated location\n",
        "    image = tf.image.pad_to_bounding_box(image, ymin, xmin, 75, 75)\n",
        "\n",
        "    # Convert image pixels to float32 and normalize them to [0, 1]\n",
        "    image = tf.cast(image, tf.float32)/255.0\n",
        "\n",
        "    # Convert coordinates to float32\n",
        "    xmin = tf.cast(xmin, tf.float32)\n",
        "    ymin = tf.cast(ymin, tf.float32)\n",
        "\n",
        "    # Calculate the normalized bounding box coordinates\n",
        "    xmax = (xmin + 28) / 75\n",
        "    ymax = (ymin + 28) / 75\n",
        "    xmin = xmin / 75\n",
        "    ymin = ymin / 75\n",
        "\n",
        "    # Return the transformed image and its corresponding label and bounding box\n",
        "    return image, (tf.one_hot(label, 10), [xmin, ymin, xmax, ymax])"
      ],
      "metadata": {
        "id": "2E5edJUwsFtn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Loads and maps the training split of the dataset using the map function. Note that we try to load the gcs version since TPU can only work with datasets on Google Cloud Storage.\n",
        "'''\n",
        "def get_training_dataset():\n",
        "    # Wrap dataset loading and preprocessing in strategy.scope() to distribute across TPUs or GPUs\n",
        "    with  strategy.scope():\n",
        "        # Load the training split of the dataset\n",
        "        dataset = tfds.load(\"mnist\", split=\"train\", as_supervised=True, try_gcs=True)\n",
        "\n",
        "        # Apply preprocessing function to each image-label pair in parallel\n",
        "        dataset = dataset.map(read_image_tfds, num_parallel_calls=16)\n",
        "\n",
        "        # Shuffle the dataset\n",
        "        dataset = dataset.shuffle(5000, reshuffle_each_iteration=True)\n",
        "\n",
        "        # Repeat the dataset indefinitely (mandatory for Keras)\n",
        "        dataset = dataset.repeat()\n",
        "\n",
        "        # Batch the dataset, dropping any remaining incomplete batches\n",
        "        dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)  # drop_remainder is important on TPU, batch size must be fixed\n",
        "\n",
        "        # Prefetch data to improve performance by overlapping data preprocessing and model execution\n",
        "        dataset = dataset.prefetch(-1)  # fetch next batches while training on the current one (-1: autotune prefetch buffer size)\n",
        "\n",
        "    return dataset\n"
      ],
      "metadata": {
        "id": "jVJfFsWOuM6Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Loads and maps the validation split of the dataset using the map function. Note that we try to load the gcs version since TPU can only work with datasets on Google Cloud Storage.\n",
        "'''\n",
        "def get_validation_dataset():\n",
        "    # Load the validation split of the MNIST dataset, attempting to load from Google Cloud Storage (GCS) for TPU compatibility\n",
        "    dataset = tfds.load(\"mnist\", split=\"test\", as_supervised=True, try_gcs=True)\n",
        "\n",
        "    # Apply preprocessing function to each image-label pair in parallel\n",
        "    dataset = dataset.map(read_image_tfds, num_parallel_calls=16)\n",
        "\n",
        "    # Batch the dataset with a batch size of 10,000, encompassing all items in the validation dataset\n",
        "    dataset = dataset.batch(10000, drop_remainder=True)\n",
        "\n",
        "    # Mandatory for Keras for now: repeat the dataset indefinitely, allowing multiple epochs during training\n",
        "    dataset = dataset.repeat()\n",
        "\n",
        "    return dataset\n"
      ],
      "metadata": {
        "id": "olJ_K4EOuj7B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the datasets within the context of the strategy scope\n",
        "with strategy.scope():\n",
        "    # Get the training dataset with preprocessing and batching suitable for TPU training\n",
        "    training_dataset = get_training_dataset()\n",
        "\n",
        "    # Get the validation dataset with preprocessing suitable for evaluation\n",
        "    validation_dataset = get_validation_dataset()"
      ],
      "metadata": {
        "id": "dZEFkh30u0Jq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualize Data"
      ],
      "metadata": {
        "id": "7a4HeW6xu7UA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pull a batch from the datasets. This code is not very nice, it gets much better in eager mode (TODO)\n",
        "def dataset_to_numpy_util(training_dataset, validation_dataset, N):\n",
        "    \"\"\"\n",
        "    Converts TensorFlow datasets to NumPy arrays.\n",
        "\n",
        "    Args:\n",
        "        training_dataset: TensorFlow dataset for training.\n",
        "        validation_dataset: TensorFlow dataset for validation.\n",
        "        N: Number of training digits to extract.\n",
        "\n",
        "    Returns:\n",
        "        training_digits: NumPy array of training digits.\n",
        "        training_labels: NumPy array of training labels.\n",
        "        training_bboxes: NumPy array of bounding boxes for training digits.\n",
        "        validation_digits: NumPy array of validation digits.\n",
        "        validation_labels: NumPy array of validation labels.\n",
        "        validation_bboxes: NumPy array of bounding boxes for validation digits.\n",
        "    \"\"\"\n",
        "    # get one batch from each: 10000 validation digits, N training digits\n",
        "    batch_train_ds = training_dataset.unbatch().batch(N)\n",
        "\n",
        "    # eager execution: loop through datasets normally\n",
        "    if tf.executing_eagerly():\n",
        "        # Extract validation data\n",
        "        for validation_digits, (validation_labels, validation_bboxes) in validation_dataset:\n",
        "            validation_digits = validation_digits.numpy()  # Convert validation digits to NumPy array\n",
        "            validation_labels = validation_labels.numpy()  # Convert validation labels to NumPy array\n",
        "            validation_bboxes = validation_bboxes.numpy()  # Convert validation bounding boxes to NumPy array\n",
        "            break\n",
        "        # Extract training data\n",
        "        for training_digits, (training_labels, training_bboxes) in batch_train_ds:\n",
        "            training_digits = training_digits.numpy()  # Convert training digits to NumPy array\n",
        "            training_labels = training_labels.numpy()  # Convert training labels to NumPy array\n",
        "            training_bboxes = training_bboxes.numpy()  # Convert training bounding boxes to NumPy array\n",
        "            break\n",
        "\n",
        "    # Decode one-hot encoded labels\n",
        "    validation_labels = np.argmax(validation_labels, axis=1)\n",
        "    training_labels = np.argmax(training_labels, axis=1)\n",
        "\n",
        "    return (training_digits, training_labels, training_bboxes,\n",
        "            validation_digits, validation_labels, validation_bboxes)\n"
      ],
      "metadata": {
        "id": "twLjMkSivNWe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Utility function to display a row of digits with their predicted bounding boxes\n",
        "def display_digits_with_boxes(digits, predictions, labels, pred_bboxes, bboxes, iou, title):\n",
        "    \"\"\"\n",
        "    Displays a row of digits with their predicted bounding boxes.\n",
        "\n",
        "    Args:\n",
        "        digits: Array of digit images.\n",
        "        predictions: Predicted labels for the digits.\n",
        "        labels: True labels for the digits.\n",
        "        pred_bboxes: Predicted bounding boxes for the digits.\n",
        "        bboxes: True bounding boxes for the digits.\n",
        "        iou: Intersection over Union values between predicted and true bounding boxes.\n",
        "        title: Title for the plot.\n",
        "    \"\"\"\n",
        "\n",
        "    n = 10  # Number of digits to display\n",
        "\n",
        "    # Randomly select n digits\n",
        "    indexes = np.random.choice(len(predictions), size=n)\n",
        "    n_digits = digits[indexes]\n",
        "    n_predictions = predictions[indexes]\n",
        "    n_labels = labels[indexes]\n",
        "\n",
        "    # Select corresponding bounding boxes and IoU values\n",
        "    n_iou = []\n",
        "    if len(iou) > 0:\n",
        "        n_iou = iou[indexes]\n",
        "\n",
        "    if len(pred_bboxes) > 0:\n",
        "        n_pred_bboxes = pred_bboxes[indexes, :]\n",
        "\n",
        "    if len(bboxes) > 0:\n",
        "        n_bboxes = bboxes[indexes, :]\n",
        "\n",
        "    # Rescale digit images to [0, 255] range\n",
        "    n_digits = n_digits * 255.0\n",
        "    n_digits = n_digits.reshape(n, 75, 75)\n",
        "\n",
        "    # Create the plot\n",
        "    fig = plt.figure(figsize=(20, 4))\n",
        "    plt.title(title)\n",
        "    plt.yticks([])\n",
        "    plt.xticks([])\n",
        "\n",
        "    # Plot each digit with its predicted and true bounding boxes\n",
        "    for i in range(10):\n",
        "        ax = fig.add_subplot(1, 10, i+1)\n",
        "        bboxes_to_plot = []\n",
        "        if len(pred_bboxes) > i:\n",
        "            bboxes_to_plot.append(n_pred_bboxes[i])\n",
        "\n",
        "        if len(bboxes) > i:\n",
        "            bboxes_to_plot.append(n_bboxes[i])\n",
        "\n",
        "        # Draw bounding boxes on the digit image\n",
        "        img_to_draw = draw_bounding_boxes_on_image_array(image=n_digits[i], boxes=np.asarray(bboxes_to_plot),\n",
        "                                                         color=['red', 'green'], display_str_list=[\"true\", \"pred\"])\n",
        "\n",
        "        plt.xlabel(n_predictions[i])\n",
        "        plt.xticks([])\n",
        "        plt.yticks([])\n",
        "\n",
        "        # Highlight incorrect predictions in red\n",
        "        if n_predictions[i] != n_labels[i]:\n",
        "            ax.xaxis.label.set_color('red')"
      ],
      "metadata": {
        "id": "dOCVlORNvlvP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(training_digits, training_labels, training_bboxes,\n",
        " validation_digits, validation_labels, validation_bboxes) = dataset_to_numpy_util(training_dataset, validation_dataset, 10)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "id": "gUvb4y00u7p_",
        "outputId": "3d6ca21c-8545-40ea-cfa9-853fd165cc8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'dataset_to_numpy_util' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-b352fe0ae345>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m (training_digits, training_labels, training_bboxes,\n\u001b[0;32m----> 2\u001b[0;31m  validation_digits, validation_labels, validation_bboxes) = dataset_to_numpy_util(training_dataset, validation_dataset, 10)\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'dataset_to_numpy_util' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "display_digits_with_boxes(training_digits, training_labels, training_labels, np.array([]), training_bboxes, np.array([]), \"training digits and their labels\")\n",
        "display_digits_with_boxes(validation_digits, validation_labels, validation_labels, np.array([]), validation_bboxes, np.array([]), \"validation digits and their labels\")"
      ],
      "metadata": {
        "id": "25h4m3rDvF5F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define the Network\n",
        "\n",
        "Here, you'll define your custom CNN.\n",
        "- `feature_extractor`: these convolutional layers extract the features of the image.\n",
        "- `classifier`:  This define the output layer that predicts among 10 categories (digits 0 through 9)\n",
        "- `bounding_box_regression`: This defines the output layer that predicts 4 numeric values, which define the coordinates of the bounding box (xmin, ymin, xmax, ymax)\n",
        "- `final_model`: This combines the layers for feature extraction, classification and bounding box prediction.  \n",
        "  - Notice that this is another example of a branching model, because the model splits to produce two kinds of output (a category and set of numbers).  \n",
        "  - Since you've learned to use the Functional API earlier in the specialization (course 1), you have the flexibility to define this kind of branching model!\n",
        "- `define_and_compile_model`: choose the optimizer and metrics, then compile the model."
      ],
      "metadata": {
        "id": "JzCKBt0zvvQ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Feature extractor is the CNN that is composed of convolution and pooling layers.\n",
        "'''\n",
        "def feature_extractor(inputs):\n",
        "    \"\"\"\n",
        "    Defines the feature extractor CNN consisting of convolution and pooling layers.\n",
        "\n",
        "    Args:\n",
        "        inputs: Input tensor.\n",
        "\n",
        "    Returns:\n",
        "        Output tensor after passing through the feature extractor CNN.\n",
        "    \"\"\"\n",
        "    # First convolutional layer with 16 filters, ReLU activation, and kernel size of 3x3\n",
        "    x = tf.keras.layers.Conv2D(16, activation='relu', kernel_size=3, input_shape=(75, 75, 1))(inputs)\n",
        "\n",
        "    # Average pooling layer with pool size of 2x2\n",
        "    x = tf.keras.layers.AveragePooling2D((2, 2))(x)\n",
        "\n",
        "    # Second convolutional layer with 32 filters, ReLU activation, and kernel size of 3x3\n",
        "    x = tf.keras.layers.Conv2D(32, kernel_size=3, activation='relu')(x)\n",
        "\n",
        "    # Average pooling layer with pool size of 2x2\n",
        "    x = tf.keras.layers.AveragePooling2D((2, 2))(x)\n",
        "\n",
        "    # Third convolutional layer with 64 filters, ReLU activation, and kernel size of 3x3\n",
        "    x = tf.keras.layers.Conv2D(64, kernel_size=3, activation='relu')(x)\n",
        "\n",
        "    # Average pooling layer with pool size of 2x2\n",
        "    x = tf.keras.layers.AveragePooling2D((2, 2))(x)\n",
        "\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "19EUyHLAvvtA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "dense_layers function adds a flatten and dense layer.\n",
        "This function follows the feature extraction layers.\n",
        "'''\n",
        "def dense_layers(inputs):\n",
        "    \"\"\"\n",
        "    Adds a flatten and dense layer to the network.\n",
        "\n",
        "    Args:\n",
        "        inputs: Input tensor.\n",
        "\n",
        "    Returns:\n",
        "        Output tensor after passing through the flatten and dense layers.\n",
        "    \"\"\"\n",
        "    # Flatten layer to convert the 3D tensor into a 1D tensor\n",
        "    x = tf.keras.layers.Flatten()(inputs)\n",
        "\n",
        "    # Dense layer with 128 units and ReLU activation\n",
        "    x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "DUaxfJaRwSap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Classifier function defines the classification output.\n",
        "This includes a set of fully connected layers followed by a softmax layer.\n",
        "'''\n",
        "def classifier(inputs):\n",
        "    \"\"\"\n",
        "    Defines the classification output of the model.\n",
        "\n",
        "    Args:\n",
        "        inputs: Input tensor.\n",
        "\n",
        "    Returns:\n",
        "        Output tensor representing the classification.\n",
        "    \"\"\"\n",
        "    # Dense layer with 10 units (for 10 classes) and softmax activation\n",
        "    classification_output = tf.keras.layers.Dense(10, activation='softmax', name='classification')(inputs)\n",
        "\n",
        "    return classification_output"
      ],
      "metadata": {
        "id": "2kRY4TjawZeq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iJOjJKIut53z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}